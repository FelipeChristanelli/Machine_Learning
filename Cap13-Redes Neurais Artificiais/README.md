# Redes Neurais Artificiais

No 13º Capítulo estudamos uma categoria bastante particular de Machine Learning que é o conjunto de Redes Neurais Artificiais.
E por que é bastante particular? 2 razões: 1º por que é uma tentativa de reproduzir o comportamento do aprendizado do cérebro humano. 2º que é praticamente um universo por si só, temos vários tipos, subtipos, combinações diferentes de Redes Neurais Artificiais e como deixamos a Rede Neural muito profunda, ou seja, com muitas camadas internas, que é o que chamamos de Deep Learning que é o estado da arte em áreas como visão computacional e PLN. 
Portanto é um tema bastante extenso e tem 3 cursos inteiros somente sobre Redes Neurais Artificiais: Deep Learning Framework, Deep Learning I e II.

Objetivos deste Capítulo
Transmitir uma visão geral de como esses modelos funcionam dividindo-o em 3 partes:
1 – Teórico
2 - Construir Modelos somente com Fórmulas Matemáticas 
3 – Mini-Projeto – Como Construir um Modelo um pouco mais complexo

Sabendo que há um universo a ser explorado:

<ul>
  <li>Introdução</li>
  <li>Introdução às Redes Neurais Artificiais</li>
  <li>O Dispositivo mais Incrível da História Humana</li>
  <li>A Origem das Redes Neurais Artificiais</li>
  <li>A Evolução das Redes Neurais Artificiais</li>
  <li>A Matemática das Redes Neurais Artificiais – Construindo a Rede Neural com Programação e Matemática</li>
  <li>A Arquitetura de Redes Neurais Artificiais</li>
  <li>Parâmetros x Hiperparâmetros</li>
  <li>Desenvolvendo a Primeira Parte da Rede – Forward Propagation</li>
  <li>Por que Inicializamos os Pesos de um Modelo de Rede Neural?</li>
  <li>Desenvolvendo a Função para Inicialização de Pesos – 2 Partes</li>
  <li>Por que Precisamos da Função de Ativação?</li>
  <li>Desenvolvendo a Função Sigmóide</li>
  <li>Afinal, O que é Derivada?</li>
  <li>Desenvolvendo a Função ReLu</li>
  <li>Desenvolvendo a Ativação Linear</li>
  <li>Construindo o Processo de Forward Propagation</li>
  <li>Combinando Ativação e Propagação</li>
  <li>Desenvolvendo a Função de Custo – Interpretando a Função de Entropia Cruzada Binária – 2 Partes</li>
  <li>Desenvolvendo a Segunda Parte da Rede – Backward Propagation</li>
  <li>Backpropagation, Gradiente Descendente e Chain Rule</li>
  <li>Desenvolvendo o Backward Propagation – Função Sigmóide Backward</li>
  <li>Desenvolvendo o Backward Propagation – Função ReLu Backward</li>
  <li>Desenvolvendo o Backward Propagation – Ativação Linear Backward</li>
  <li>Combinando Ativação e Retropropagação – Algoritmo Backpropagation</li>
  <li>Gradientes e Atualização dos Pesos</li>
  <li>Implementando a Rede Completa</li>
  <li>Mini-Projeto 4 – Usando a Rede Neural para Prever a Ocorrência de Câncer </li>
  <li>Mini-Projeto 5 – Rede Neural com TensorFlow para Classificação de Imagens de Vestuário</li>
  <li>Estudo de Caso – Prevendo os Efeitos do Consumo de Álcool em Doenças do Fígado</li>
</ul>